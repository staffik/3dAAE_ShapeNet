{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from plyfile import PlyData, PlyElement\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='datasets/shapenet/03001627'\n",
    "\n",
    "def read_point_cloud(filename):\n",
    "    plydata = PlyData.read(filename)\n",
    "    x = np.array(plydata['vertex']['x'])\n",
    "    y = np.array(plydata['vertex']['y'])\n",
    "    z = np.array(plydata['vertex']['z'])\n",
    "\n",
    "    return np.transpose(np.array([x,y,z]))\n",
    "\n",
    "def read_point_clouds(path):\n",
    "    fns = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            fns.append(os.path.join(root, file))\n",
    "    return [read_point_cloud(fn) for fn in fns]\n",
    "\n",
    "point_clouds = read_point_clouds(DATA_PATH)\n",
    "print('{} point clouds of shape {}'.format(len(point_clouds), point_clouds[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_point = 2048\n",
    "point_dim = 3\n",
    "\n",
    "point_clouds_data = np.concatenate(point_clouds).reshape(-1, num_point, point_dim, 1)\n",
    "print(point_clouds_data.shape)\n",
    "\n",
    "train_X, test_X, train_ground, test_ground = train_test_split(point_clouds_data, point_clouds_data,\n",
    "                                                              test_size=0.15, random_state=42)\n",
    "train_X, valid_X, train_ground, valid_ground = train_test_split(train_X, train_ground,\n",
    "                                                              test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "\n",
    "def get_mEncoder():\n",
    "    net = keras.models.Sequential()\n",
    "    input_shape = (num_point, point_dim, 1)\n",
    "    net.add(keras.layers.Conv2D(64, (1, 3), strides=(1,1), activation='relu', padding='valid', input_shape=input_shape))\n",
    "    net.add(keras.layers.Conv2D(64, (1, 1), strides=(1,1), activation='relu', padding='valid'))\n",
    "    net.add(keras.layers.Conv2D(64, (1, 1), strides=(1,1), activation='relu', padding='valid'))\n",
    "    net.add(keras.layers.Conv2D(128, (1, 1), strides=(1,1), activation='relu', padding='valid'))\n",
    "    net.add(keras.layers.Conv2D(latent_dim, (1, 1), strides=(1,1), activation='relu', padding='valid'))\n",
    "    net.add(keras.layers.MaxPool2D((num_point, 1), strides=(2,2), padding='valid'))\n",
    "    net.add(keras.layers.Reshape((-1,)))\n",
    "    \n",
    "    return net\n",
    "\n",
    "def get_mDecoder():\n",
    "    net = keras.models.Sequential()\n",
    "    input_shape = (latent_dim,)\n",
    "    net.add(keras.layers.Dense(1024, activation='relu', input_shape=input_shape))\n",
    "    net.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    net.add(keras.layers.Dense(num_point*3, activation=None))\n",
    "    net.add(keras.layers.Reshape((num_point, 3, 1)))\n",
    "    \n",
    "    return net\n",
    "\n",
    "def get_mDiscriminator():\n",
    "    net = keras.models.Sequential()\n",
    "    input_shape = (latent_dim,)\n",
    "    net.add(keras.layers.Dense(1024, activation='relu', input_shape=input_shape))\n",
    "    net.add(keras.layers.Dense(1024, activation='relu'))\n",
    "    net.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return net\n",
    "\n",
    "def get_mAE():\n",
    "    mEncoder = get_mEncoder()\n",
    "    mDecoder = get_mDecoder()\n",
    "    \n",
    "    mAE = keras.models.Sequential()\n",
    "    mAE.add(mEncoder)\n",
    "    mAE.add(mDecoder)\n",
    "    \n",
    "    return mEncoder, mDecoder, mAE\n",
    "\n",
    "def get_mAAE():\n",
    "    mEncoder, mDecoder, mAE = get_mAE()\n",
    "    mDiscriminator = get_mDiscriminator()\n",
    "    \n",
    "    mGAN = keras.models.Sequential()\n",
    "    mGAN.add(mEncoder)\n",
    "    mGAN.add(mDiscriminator)\n",
    "    \n",
    "    return mEncoder, mDecoder, mDiscriminator, mAE, mGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def row_norms(X):\n",
    "    return K.sum(K.square(X), axis=-1)\n",
    "\n",
    "def chamfer_loss(y_true, y_pred):\n",
    "    Y = K.reshape(y_true, (-1, 2048, 3))\n",
    "    X = K.reshape(y_pred, (-1, 2048, 3))\n",
    "    \n",
    "    XX = row_norms(X)\n",
    "    YY = row_norms(Y)\n",
    "    D = K.batch_dot(X, K.permute_dimensions(Y,(0,2,1)))\n",
    "    \n",
    "    D *= -2\n",
    "    D += K.repeat_elements(K.expand_dims(XX, axis=-1), 2048, 2)\n",
    "    D += K.repeat(YY, 2048)\n",
    "    \n",
    "    x_to_y = K.sum(K.min(D, axis=-1))\n",
    "    y_to_x = K.sum(K.min(K.permute_dimensions(D,(0,2,1)), axis=-1))\n",
    "    \n",
    "    return  x_to_y + y_to_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_SAVED_MODELS = 1\n",
    "path_to_models = 'models/'\n",
    "\n",
    "def load_saved_models(models_names):\n",
    "    models = []\n",
    "    for model_name in models_names:\n",
    "        json_file = open(path_to_models+model_name+'.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(path_to_models+model_name+'.h5')\n",
    "        print(\"Loaded model {} from disk\".format(model_name))\n",
    "        models.append(loaded_model)\n",
    "    return models\n",
    "\n",
    "if LOAD_SAVED_MODELS:\n",
    "    mEncoder, mDecoder, mDiscriminator, mAE, mGAN, model_autoencoder = \\\n",
    "                load_saved_models(['enc','dec','disc','ae','enc_disc', 'autoencoder'])\n",
    "else:\n",
    "    mEncoder, mDecoder, mDiscriminator, mAE, mGAN = get_mAAE()\n",
    "    _, _, model_autoencoder = get_mAE() \n",
    "\n",
    "models = {\"enc\": mEncoder,\n",
    "          \"dec\": mDecoder,\n",
    "          \"disc\": mDiscriminator,\n",
    "          \"ae\": mAE,\n",
    "          \"enc_disc\": mGAN,\n",
    "          \"autoencoder\": model_autoencoder}\n",
    "\n",
    "model_autoencoder.compile(loss=chamfer_loss, optimizer = keras.optimizers.Adam(lr=1e-3))\n",
    "\n",
    "mDiscriminator.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss=\"binary_crossentropy\")\n",
    "mAE.compile(optimizer = keras.optimizers.Adam(), loss=chamfer_loss)\n",
    "mGAN.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss=\"binary_crossentropy\")\n",
    "\n",
    "\n",
    "ae_encoder = keras.models.Model(inputs=model_autoencoder.input,\n",
    "                             outputs=model_autoencoder.layers[0].get_output_at(0))\n",
    "ae_decoder = keras.models.Model(inputs=model_autoencoder.layers[1].get_input_at(0),\n",
    "                              outputs=model_autoencoder.layers[1].get_output_at(0))\n",
    "\n",
    "aae_encoder = keras.models.Model(inputs=mAE.input, outputs=mAE.layers[0].get_output_at(0))\n",
    "aae_decoder = keras.models.Model(inputs=mAE.layers[1].get_input_at(0),\n",
    "                          outputs=mAE.layers[1].get_output_at(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AE = 0\n",
    "\n",
    "if TRAIN_AE:\n",
    "    bs = 64 # size of batch\n",
    "    epochs = 650\n",
    "    autoencoder_train = model_autoencoder.fit(train_X, train_ground, batch_size=bs, epochs=epochs, verbose=1, validation_data=(valid_X, valid_ground))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AAE = 0\n",
    "\n",
    "def setTrainMode(net, value):\n",
    "    for L in net.layers:\n",
    "        L.trainable = value\n",
    "    net.trainable = value\n",
    "    \n",
    "def setTrainModes(nets, values):\n",
    "    for N, V in zip(nets, values):\n",
    "        setTrainMode(N, V)\n",
    "\n",
    "if TRAIN_AAE:\n",
    "    bs = 64 # size of batch\n",
    "    epochs = 650\n",
    "    batch_num = int(train_X.shape[0] / bs)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch #\", epoch)\n",
    "        np.random.shuffle(train_X)\n",
    "\n",
    "        for i in range(batch_num):\n",
    "            batchData = train_X[bs*i : bs*(i+1)]\n",
    "            \n",
    "            setTrainModes([mAE, mEncoder, mDecoder], [1,1,1])\n",
    "            mAE.train_on_batch(batchData, batchData)\n",
    "\n",
    "            data_repr = mEncoder.predict(batchData)\n",
    "            sample = np.random.standard_normal((bs, latent_dim))\n",
    "            inputDisc = np.concatenate([sample, data_repr])\n",
    "            labelsDisc = np.concatenate([np.ones(bs), np.zeros(bs)])\n",
    "            \n",
    "            setTrainMode(mDiscriminator, True)\n",
    "            mDiscriminator.train_on_batch(inputDisc, labelsDisc)\n",
    "\n",
    "            setTrainModes([mGAN, mEncoder, mDiscriminator], [1,1,0])\n",
    "            mGAN.train_on_batch(batchData, np.ones(bs))\n",
    "\n",
    "        lossAE = mAE.evaluate(train_X, train_X, verbose=0)\n",
    "        lossGAN = mGAN.evaluate(train_X, np.ones(train_X.shape[0]), verbose=0)\n",
    "        print(\"AE part's loss: {}\\nGAN part's loss: {}\".format(lossAE, lossGAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TRAINED_MODELS = 0\n",
    "\n",
    "def save_models():\n",
    "    for model_name in models:\n",
    "        model = models[model_name]\n",
    "        model_json = model.to_json()\n",
    "        with open(path_to_models+model_name+\".json\", \"w+\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(path_to_models+model_name+\".h5\")\n",
    "        print(\"Saved model {} to disk\".format(model_name))\n",
    "\n",
    "if SAVE_TRAINED_MODELS:\n",
    "    save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chamfer_distance(A, B):\n",
    "    A = np.reshape(A, (-1, point_dim))\n",
    "    B = np.reshape(B, (-1, point_dim))\n",
    "    AA = np.sum(A**2, axis=1)\n",
    "    BB = np.sum(B**2, axis=1)\n",
    "    D = AA[:, np.newaxis] - 2*np.dot(A, B.T) + BB[np.newaxis, :]\n",
    "    return np.sum(np.min(D, axis=0)) + np.sum(np.min(D, axis=1))\n",
    "    \n",
    "def calculate_COV(A, B):\n",
    "    covered = set()\n",
    "    for a in A:\n",
    "        covered.add(np.argmin(np.array([chamfer_distance(a, b) for b in B])))\n",
    "    return len(covered) * 100.0 / len(B)\n",
    "\n",
    "def calculate_MMD(A, B):\n",
    "    sum_d = 0.0\n",
    "    for b in B:\n",
    "        sum_d += np.min(np.array([chamfer_distance(a, b) for a in A]))\n",
    "    return sum_d * 1.0 / len(B)\n",
    "\n",
    "def show_point_cloud(pcl):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(pcl[:,0], pcl[:,1], pcl[:,2], c='b')\n",
    "    plt.show()\n",
    "\n",
    "def decode_and_show(decoder, representation):\n",
    "    show_point_cloud(decoder.predict(representation)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RECONSTRUCTION = 0\n",
    "\n",
    "def evaluate_reconstruction(method, input_data):\n",
    "    eval_size = len(input_data)\n",
    "    sum_dist = 0\n",
    "    for i in range(eval_size):\n",
    "        pred = method(np.array([input_data[i]]))\n",
    "        sum_dist += chamfer_distance(input_data[i], pred[0])\n",
    "    mean_sum = sum_dist / eval_size\n",
    "    print(\"Mean reconstruction distance: {}\".format(mean_sum))\n",
    "\n",
    "if TEST_RECONSTRUCTION:\n",
    "    evaluate_reconstruction(mAE.predict, train_X)\n",
    "    evaluate_reconstruction(mAE.predict, test_X)\n",
    "    evaluate_reconstruction(model_autoencoder.predict, train_X)\n",
    "    evaluate_reconstruction(model_autoencoder.predict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REPRESENTATION = 0\n",
    "\n",
    "def eval_representation(A, B):\n",
    "    COV = calculate_COV(A, B)\n",
    "    MMD = calculate_MMD(A, B)\n",
    "    print(\"COV = {}, MMD = {}\".format(COV, MMD))\n",
    "\n",
    "if TEST_REPRESENTATION:\n",
    "    sample = lambda arr : np.random.permutation(arr)[ : int(arr.shape[0] * 0.98)]\n",
    "\n",
    "    ground_truth = test_X\n",
    "    sampled = np.array([sample(arr) for arr in ground_truth])\n",
    "    reconstructed_ae = model_autoencoder.predict(ground_truth)\n",
    "    reconstructed_aae = mAE.predict(ground_truth)\n",
    "\n",
    "    eval_representation(ground_truth, ground_truth)\n",
    "    eval_representation(ground_truth, sampled)\n",
    "    eval_representation(ground_truth, reconstructed_ae)\n",
    "    eval_representation(ground_truth, reconstructed_aae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ARITHMETIC_OF_LATENT_SPACE = 0\n",
    "\n",
    "if TEST_ARITHMETIC_OF_LATENT_SPACE:\n",
    "    chair1 = train_X[34:35]\n",
    "    chair2 = train_X[3:4]\n",
    "    repr1 = aae_encoder.predict(chair1)\n",
    "    repr2 = aae_encoder.predict(chair2)\n",
    "\n",
    "    decode_and_show(aae_decoder, repr1)\n",
    "    decode_and_show(aae_decoder, repr2)\n",
    "    decode_and_show(aae_decoder, (repr1 + repr2) * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INTERPOLATION = 0\n",
    "\n",
    "def interpolate(decoder, repr1, repr2, k):\n",
    "    interpolations = []\n",
    "    for i in range(k+1):\n",
    "        representation = repr1 + i*(repr2-repr1)/k\n",
    "        decoded = decoder.predict(representation)[0]\n",
    "        show_point_cloud(decoded)\n",
    "        interpolations.append(representation[0])\n",
    "    \n",
    "    return np.array(interpolations)\n",
    "        \n",
    "if TEST_INTERPOLATION:\n",
    "    chair1 = train_X[34:35]\n",
    "    chair2 = train_X[82:83]\n",
    "    repr1 = aae_encoder.predict(chair1)\n",
    "    repr2 = aae_encoder.predict(chair2)\n",
    "    \n",
    "    k = 7\n",
    "    interpolations = interpolate(aae_decoder, repr1, repr2, k)\n",
    "    pca_data = aae_encoder.predict(train_X)\n",
    "    \n",
    "    pca = PCA(n_components=2, svd_solver='arpack')\n",
    "    proj_train = pca.fit_transform(pca_data)\n",
    "    proj_interps = pca.transform(interpolations)\n",
    "    \n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.scatter(proj_train[:, 0], proj_train[:, 1], c='b')\n",
    "    ax.scatter(proj_interps[:, 0], proj_interps[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SHAPE_COMPLETION = 0\n",
    "\n",
    "if TEST_SHAPE_COMPLETION:\n",
    "    chair = train_X[32:33]\n",
    "    partial_chair = chair.copy()\n",
    "\n",
    "    points_cnt=0\n",
    "    last_point = [[0.2],[-0.1],[-0.3]]\n",
    "    for idx, i in enumerate(partial_chair[0]):\n",
    "        if np.linalg.norm(i-[[0.2],[-0.1],[-0.3]])<0.33:\n",
    "            partial_chair[0][idx] = last_point\n",
    "            points_cnt+=1\n",
    "        else:\n",
    "            last_point = partial_chair[0][idx]     \n",
    "    removed_percent = points_cnt*100.0/len(partial_chair[0])\n",
    "    print(\"Removed points: {}%\".format(removed_percent))\n",
    "    \n",
    "    repr_part_chair = aae_encoder.predict(partial_chair)\n",
    "    completed_chair = aae_decoder.predict(repr_part_chair)\n",
    "    reconstructed_chair = mAE.predict(chair)\n",
    "    \n",
    "    show_point_cloud(chair[0])\n",
    "    show_point_cloud(partial_chair[0])\n",
    "    show_point_cloud(completed_chair[0])\n",
    "    show_point_cloud(reconstructed_chair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_GENERATING = 0\n",
    "\n",
    "def calc_used_features(preds):\n",
    "    used_features_cnt = 0\n",
    "    for i in range(latent_dim):\n",
    "        cnt = 0\n",
    "        for j in preds:\n",
    "            if j[i]>0:\n",
    "                cnt+=1\n",
    "        print(\"{}: {}%\".format(i,cnt*100/len(train_X)))\n",
    "        if(cnt>0):\n",
    "            used_features_cnt += 1\n",
    "    print(\"#used features = {}\".format(used_features_cnt))\n",
    "\n",
    "if TEST_GENERATING:\n",
    "    repr_fake = np.random.standard_normal((1, latent_dim))\n",
    "    repr_fake = repr_fake / np.linalg.norm(repr_fake)\n",
    "    \n",
    "    repr_real = aae_encoder.predict(train_X[32:33])\n",
    "    repr_mix = (repr_real + repr_fake) / 2\n",
    "    repr_zero = np.zeros((1,latent_dim))\n",
    "    \n",
    "    pred_fake = aae_decoder.predict(repr_fake)[0]\n",
    "    pred_real = aae_decoder.predict(repr_real)[0]\n",
    "    pred_mix = aae_decoder.predict(repr_mix)[0]\n",
    "    pred_zero_ae = ae_decoder.predict(repr_zero)[0]\n",
    "    pred_zero_aae = aae_decoder.predict(repr_zero)[0]\n",
    "    \n",
    "    show_point_cloud(pred_fake)\n",
    "    show_point_cloud(pred_real)\n",
    "    show_point_cloud(pred_mix)\n",
    "    show_point_cloud(pred_zero_ae)\n",
    "    show_point_cloud(pred_zero_aae)\n",
    "    \n",
    "    idxs = [25,21,6,1,2]\n",
    "\n",
    "    for i in idxs:\n",
    "        chair = train_X[i:i+1]\n",
    "        show_point_cloud(chair[0])\n",
    "        code = aae_encoder.predict(chair)\n",
    "        print(code)\n",
    "    \n",
    "    preds_ae = ae_encoder.predict(train_X)\n",
    "    preds_aae = aae_encoder.predict(train_X)\n",
    "    calc_used_features(preds_ae)\n",
    "    calc_used_features(preds_aae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
